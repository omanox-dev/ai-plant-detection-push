{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ed92c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#import openai\n",
    "#import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3591f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Removed 0 corrupted images.\n",
      "âœ… Removed 0 duplicate images.\n",
      "âœ… All images resized to (224, 224) and saved in 'C:/Users/Ayush/ath_codee/new_custom_cleaned_dataset'.\n",
      "âœ… Data normalization (rescaling 1./255) will be applied during training.\n",
      "Found 72 images belonging to 27 classes.\n",
      "Found 0 images belonging to 27 classes.\n",
      "âœ… Data augmentation pipeline ready (rotation, flip, zoom, brightness, etc.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "SOURCE_DIR = \"C:/Users/Ayush/ath_codee/2_custom_plant_disease_dataset__\"            # path to your raw dataset\n",
    "CLEANED_DIR = \"C:/Users/Ayush/ath_codee/new_custom_cleaned_dataset\"   # output folder for cleaned dataset\n",
    "IMG_SIZE = (224, 224)                    # target size for CNN\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1ï¸âƒ£ Remove Corrupted Images\n",
    "# ----------------------------\n",
    "def remove_corrupted_images(source_dir):\n",
    "    removed = 0\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img.verify()  # verifies if image is readable\n",
    "            except Exception:\n",
    "                print(f\"âŒ Removing corrupted image: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "                removed += 1\n",
    "    print(f\"âœ… Removed {removed} corrupted images.\")\n",
    "\n",
    "remove_corrupted_images(SOURCE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 2ï¸âƒ£ Remove Duplicate Images\n",
    "# ----------------------------\n",
    "def remove_duplicate_images(source_dir):\n",
    "    seen_hashes = set()\n",
    "    duplicates = 0\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            if file_hash in seen_hashes:\n",
    "                print(f\"ğŸ—‘ Removing duplicate image: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "                duplicates += 1\n",
    "            else:\n",
    "                seen_hashes.add(file_hash)\n",
    "    print(f\"âœ… Removed {duplicates} duplicate images.\")\n",
    "\n",
    "remove_duplicate_images(SOURCE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 3ï¸âƒ£ Resize Images\n",
    "# ----------------------------\n",
    "def resize_images(source_dir, output_dir, target_size=(224, 224)):\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        output_folder = os.path.join(output_dir, folder)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        for file in os.listdir(folder_path):\n",
    "            src_path = os.path.join(folder_path, file)\n",
    "            dst_path = os.path.join(output_folder, file)\n",
    "            try:\n",
    "                img = Image.open(src_path).convert(\"RGB\")\n",
    "                img = img.resize(target_size)\n",
    "                img.save(dst_path)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error resizing {file}: {e}\")\n",
    "    print(f\"âœ… All images resized to {target_size} and saved in '{output_dir}'.\")\n",
    "\n",
    "resize_images(SOURCE_DIR, CLEANED_DIR, IMG_SIZE)\n",
    "\n",
    "# ----------------------------\n",
    "# 4ï¸âƒ£ Data Normalization (for model input)\n",
    "# ----------------------------\n",
    "# Note: We'll apply normalization dynamically using ImageDataGenerator below.\n",
    "# This step ensures pixel values are scaled between 0 and 1.\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "print(\"âœ… Data normalization (rescaling 1./255) will be applied during training.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5ï¸âƒ£ Data Augmentation\n",
    "# ----------------------------\n",
    "# Youâ€™ll apply this dynamically during training for better generalization.\n",
    "# Example augmentations for plant leaf datasets:\n",
    "\n",
    "aug_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.15  # 85% training, 15% validation\n",
    ")\n",
    "\n",
    "# Example usage â€” generator creation\n",
    "train_gen = aug_datagen.flow_from_directory(\n",
    "    CLEANED_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_gen = aug_datagen.flow_from_directory(\n",
    "    CLEANED_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "print(\"âœ… Data augmentation pipeline ready (rotation, flip, zoom, brightness, etc.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81ce2cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Aloe Leaf spot: 2 train, 0 val, 1 test\n",
      "âœ… Aloe Soft rot: 1 train, 0 val, 1 test\n",
      "âœ… Arali Leaf blight: 2 train, 0 val, 1 test\n",
      "âœ… Bamboo Rust: 1 train, 0 val, 1 test\n",
      "âœ… banana Panama wilt: 1 train, 0 val, 1 test\n",
      "âœ… banana Sigatoka: 2 train, 0 val, 1 test\n",
      "âœ… Chilli Leaf curl virus: 1 train, 0 val, 1 test\n",
      "âœ… Curry leaf black spot: 1 train, 0 val, 1 test\n",
      "âœ… Ginger Leaf spot: 2 train, 0 val, 1 test\n",
      "âœ… Guava Anthracnose: 0 train, 0 val, 0 test\n",
      "âœ… Guava Rust: 2 train, 0 val, 1 test\n",
      "âœ… Hibiscus Leaf spot: 1 train, 0 val, 1 test\n",
      "âœ… Hibiscus Rust: 3 train, 0 val, 1 test\n",
      "âœ… Jasmine Leaf spot: 1 train, 0 val, 1 test\n",
      "âœ… Jasmine Rust: 2 train, 0 val, 1 test\n",
      "âœ… Lemon grass Helminthosporium spot: 2 train, 0 val, 1 test\n",
      "âœ… Lemon grass Leaf blight: 2 train, 0 val, 1 test\n",
      "âœ… Lemon grass Rust: 1 train, 0 val, 1 test\n",
      "âœ… Mint Leaf Blight: 2 train, 0 val, 1 test\n",
      "âœ… Mint Rust: 3 train, 0 val, 1 test\n",
      "âœ… Neem Powdery mildew: 0 train, 0 val, 1 test\n",
      "âœ… Pepper Phytophthora foot rot: 1 train, 0 val, 1 test\n",
      "âœ… Powdery mildew: 4 train, 0 val, 1 test\n",
      "âœ… Rose Black spot: 1 train, 0 val, 1 test\n",
      "âœ… Sooty mold: 3 train, 0 val, 1 test\n",
      "âœ… Tumeric Leaf blotch: 3 train, 0 val, 1 test\n",
      "âœ… Tumeric Leaf spot: 2 train, 0 val, 1 test\n",
      "\n",
      "ğŸ¯ Data successfully split and saved to 'custom_split_dataset/'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths\n",
    "SOURCE_DIR = \"C:/Users/Ayush/ath_codee/2_custom_plant_disease_dataset__\"\n",
    "DEST_DIR = \"C:/Users/Ayush/ath_codee/new_custom_split_dataset\"\n",
    "\n",
    "# Ratios\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# Create destination folders\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(DEST_DIR, split), exist_ok=True)\n",
    "\n",
    "# Split each class folder\n",
    "for class_name in os.listdir(SOURCE_DIR):\n",
    "    class_path = os.path.join(SOURCE_DIR, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    \n",
    "    # Destination class folders\n",
    "    train_folder = os.path.join(DEST_DIR, \"train\", class_name)\n",
    "    val_folder = os.path.join(DEST_DIR, \"val\", class_name)\n",
    "    test_folder = os.path.join(DEST_DIR, \"test\", class_name)\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "    \n",
    "    # Get image files\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calculate split indices\n",
    "    total = len(images)\n",
    "    train_end = int(total * TRAIN_SPLIT)\n",
    "    val_end = train_end + int(total * VAL_SPLIT)\n",
    "\n",
    "    # Split lists\n",
    "    train_imgs = images[:train_end]\n",
    "    val_imgs = images[train_end:val_end]\n",
    "    test_imgs = images[val_end:]\n",
    "\n",
    "    # Copy images\n",
    "    for img in train_imgs:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(train_folder, img))\n",
    "    for img in val_imgs:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(val_folder, img))\n",
    "    for img in test_imgs:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(test_folder, img))\n",
    "    \n",
    "    print(f\"âœ… {class_name}: {len(train_imgs)} train, {len(val_imgs)} val, {len(test_imgs)} test\")\n",
    "\n",
    "print(\"\\nğŸ¯ Data successfully split and saved to 'custom_split_dataset/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"C:/Users/Ayush/ath_codee/them...........DATASETS!!/2_custom_cleaned_dataset/custom_split_dataset/train\"\n",
    "val_dir = \"C:/Users/Ayush/ath_codee/them...........DATASETS!!/2_custom_cleaned_dataset/custom_split_dataset/val\"\n",
    "test_dir = \"C:/Users/Ayush/ath_codee/them...........DATASETS!!/2_custom_cleaned_dataset/custom_split_dataset/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9104562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 images belonging to 27 classes.\n",
      "Found 0 images belonging to 27 classes.\n",
      "Found 26 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = aug_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_gen = aug_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_gen = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f45d8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "Train dir: path_to_train_dir\n",
      "Validation dir: path_to_val_dir\n",
      "Test dir: path_to_test_dir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(\n",
    "    source_dir, train_dir, val_dir, test_dir,\n",
    "    train_split=0.7, val_split=0.15, test_split=0.15,\n",
    "    seed=42\n",
    "):\n",
    "    random.seed(seed)\n",
    "    # Create directories if not exists\n",
    "    for d in [train_dir, val_dir, test_dir]:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "\n",
    "    classes = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(source_dir, cls)\n",
    "        images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        n_total = len(images)\n",
    "        n_train = int(n_total * train_split)\n",
    "        n_val = int(n_total * val_split)\n",
    "        n_test = n_total - n_train - n_val  # rest\n",
    "\n",
    "        # Make class-wise directories in train/val/test\n",
    "        for split_dir in [train_dir, val_dir, test_dir]:\n",
    "            os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n",
    "\n",
    "        # Copy files to respective dirs\n",
    "        for i, img in enumerate(images):\n",
    "            src_path = os.path.join(cls_path, img)\n",
    "            if i < n_train:\n",
    "                dst_path = os.path.join(train_dir, cls, img)\n",
    "            elif i < (n_train + n_val):\n",
    "                dst_path = os.path.join(val_dir, cls, img)\n",
    "            else:\n",
    "                dst_path = os.path.join(test_dir, cls, img)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"Dataset split complete:\")\n",
    "    print(f\"Train dir: {train_dir}\")\n",
    "    print(f\"Validation dir: {val_dir}\")\n",
    "    print(f\"Test dir: {test_dir}\")\n",
    "\n",
    "# Example usage:\n",
    "source_dataset_dir = 'C:/Users/Ayush/ath_codee/2_custom_plant_disease_dataset__'\n",
    "train_directory = 'path_to_train_dir'\n",
    "val_directory = 'path_to_val_dir'\n",
    "test_directory = 'path_to_test_dir'\n",
    "\n",
    "split_dataset(source_dataset_dir, train_directory, val_directory, test_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "e82e0462",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded pretrained CNN model successfully!\n",
      "Found 46 images belonging to 27 classes.\n",
      "Found 0 images belonging to 27 classes.\n",
      "Found 26 images belonging to 27 classes.\n",
      "ğŸŒ¿ Custom dataset has 27 classes.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'models' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[10]\u001b[39m\u001b[32m, line 78\u001b[39m\n\u001b[32m     75\u001b[39m base_model = model.layers[\u001b[32m0\u001b[39m]  \u001b[38;5;66;03m# EfficientNet base\u001b[39;00m\n\u001b[32m     76\u001b[39m base_model.trainable = \u001b[38;5;28;01mFalse\u001b[39;00m  \u001b[38;5;66;03m# freeze for initial training\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m78\u001b[39m new_model = \u001b[43mmodels\u001b[49m.Sequential([\n\u001b[32m     79\u001b[39m     base_model,\n\u001b[32m     80\u001b[39m     layers.GlobalAveragePooling2D(),\n\u001b[32m     81\u001b[39m     layers.Dropout(\u001b[32m0.3\u001b[39m),\n\u001b[32m     82\u001b[39m     layers.Dense(\u001b[32m256\u001b[39m, activation=\u001b[33m'\u001b[39m\u001b[33mrelu\u001b[39m\u001b[33m'\u001b[39m),\n\u001b[32m     83\u001b[39m     layers.BatchNormalization(),\n\u001b[32m     84\u001b[39m     layers.Dropout(\u001b[32m0.3\u001b[39m),\n\u001b[32m     85\u001b[39m     layers.Dense(num_new_classes, activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m)\n\u001b[32m     86\u001b[39m ])\n\u001b[32m     88\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     89\u001b[39m \u001b[38;5;66;03m# 5ï¸âƒ£ Compile Model\u001b[39;00m\n\u001b[32m     90\u001b[39m \u001b[38;5;66;03m# ----------------------------\u001b[39;00m\n\u001b[32m     91\u001b[39m new_model.compile(\n\u001b[32m     92\u001b[39m     optimizer=tf.keras.optimizers.Adam(learning_rate=\u001b[32m0.001\u001b[39m),\n\u001b[32m     93\u001b[39m     loss=\u001b[33m'\u001b[39m\u001b[33mcategorical_crossentropy\u001b[39m\u001b[33m'\u001b[39m,\n\u001b[32m     94\u001b[39m     metrics=[\u001b[33m'\u001b[39m\u001b[33maccuracy\u001b[39m\u001b[33m'\u001b[39m]\n\u001b[32m     95\u001b[39m )\n",
      "\u001b[31mNameError\u001b[39m: name 'models' is not defined"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# 1ï¸âƒ£ Load the Pretrained Model\n",
    "# ----------------------------\n",
    "# Path to your pretrained model\n",
    "MODEL_PATH = \"C:/Users/Ayush/ath_codee/1_CNN_MODEL_code/efficientnetb0_plant_species.h5\"  # or plant_classifier_dataframe.h5\n",
    "model = load_model(MODEL_PATH)\n",
    "print(\"Loaded pretrained CNN model successfully!\")\n",
    "\n",
    "# ----------------------------\n",
    "# 2ï¸âƒ£ Prepare Custom Dataset Paths\n",
    "# ----------------------------\n",
    "train_dir = \"C:/Users/Ayush/ath_codee/new_custom_split_dataset/train\"\n",
    "val_dir = \"C:/Users/Ayush/ath_codee/new_custom_split_dataset/val\"\n",
    "test_dir = \"C:/Users/Ayush/ath_codee/new_custom_split_dataset/test\"\n",
    "\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# ----------------------------\n",
    "# 3ï¸âƒ£ Set Up Data Generators\n",
    "# ----------------------------\n",
    "# Augmentation + normalization\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2]\n",
    ")\n",
    "\n",
    "val_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "test_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input\n",
    ")\n",
    "\n",
    "# Generators\n",
    "train_gen = train_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_gen = val_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_gen = test_datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 4ï¸âƒ£ Adjust the Model Output Layer\n",
    "# ----------------------------\n",
    "# The old model was trained on N classes (medicinal species)\n",
    "# The new dataset has possibly different class count\n",
    "\n",
    "num_new_classes = len(train_gen.class_indices)\n",
    "print(f\"ğŸŒ¿ Custom dataset has {num_new_classes} classes.\")\n",
    "\n",
    "# Remove old top layers and add new classification head\n",
    "base_model = model.layers[0]  # EfficientNet base\n",
    "base_model.trainable = False  # freeze for initial training\n",
    "\n",
    "new_model = models.Sequential([\n",
    "    base_model,\n",
    "    layers.GlobalAveragePooling2D(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(256, activation='relu'),\n",
    "    layers.BatchNormalization(),\n",
    "    layers.Dropout(0.3),\n",
    "    layers.Dense(num_new_classes, activation='softmax')\n",
    "])\n",
    "\n",
    "# ----------------------------\n",
    "# 5ï¸âƒ£ Compile Model\n",
    "# ----------------------------\n",
    "new_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 6ï¸âƒ£ Train the Model (Transfer Learning Phase)\n",
    "# ----------------------------\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1),\n",
    "    ModelCheckpoint(\"fine_tuned_model_stage1.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "history_stage1 = new_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 7ï¸âƒ£ Fine-Tune Stage (Unfreeze Top Layers)\n",
    "# ----------------------------\n",
    "base_model.trainable = True\n",
    "for layer in base_model.layers[:-20]:\n",
    "    layer.trainable = False\n",
    "\n",
    "new_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "callbacks_stage2 = [\n",
    "    EarlyStopping(monitor='val_accuracy', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.3, patience=3, verbose=1),\n",
    "    ModelCheckpoint(\"fine_tuned_model_final.h5\", save_best_only=True)\n",
    "]\n",
    "\n",
    "history_stage2 = new_model.fit(\n",
    "    train_gen,\n",
    "    validation_data=val_gen,\n",
    "    epochs=10,\n",
    "    callbacks=callbacks_stage2\n",
    ")\n",
    "\n",
    "# ----------------------------\n",
    "# 8ï¸âƒ£ Evaluate on Test Set\n",
    "# ----------------------------\n",
    "test_loss, test_acc = new_model.evaluate(test_gen)\n",
    "print(f\" Final Model Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "\n",
    "# ----------------------------\n",
    "# 9ï¸âƒ£ Save Final Model\n",
    "# ----------------------------\n",
    "new_model.save(\"custom_plant_disease_classifier.h5\")\n",
    "print(\"Fine-tuned custom disease model saved as 'New_custom_plant_disease_classifier.h5'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074603e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# -----------------------------\n",
    "# 1ï¸âƒ£ Setup OpenAI API key\n",
    "# -----------------------------\n",
    "# Make sure to set your API key as environment variable before running:\n",
    "# export OPENAI_API_KEY=\"your_api_key_here\"\n",
    "openai.api_key = os.getenv(\"OPENAI_API_KEY\")\n",
    "\n",
    "# -----------------------------\n",
    "# 2ï¸âƒ£ Load CNN Model & Labels\n",
    "# -----------------------------\n",
    "model = load_model(\"C:/Users/Ayush/ath_codee/1_CNN_MODEL_code/custom_plant_disease_classifier.h5\")\n",
    "\n",
    "# -----------------------------\n",
    "# 3ï¸âƒ£ Preprocess and Predict\n",
    "# -----------------------------\n",
    "def preprocess_image(img_path, target_size=(224, 224)):\n",
    "    img = cv2.imread(img_path)\n",
    "    if img is None:\n",
    "        raise ValueError(f\"âŒ Image not found: {img_path}\")\n",
    "    img = cv2.cvtColor(img, cv2.COLOR_BGR2RGB)\n",
    "    img = cv2.resize(img, target_size)\n",
    "    img = img.astype(\"float32\") / 255.0\n",
    "    img = np.expand_dims(img, axis=0)\n",
    "    return img\n",
    "\n",
    "def predict_plant_disease(img_path):\n",
    "    img = preprocess_image(img_path)\n",
    "    preds = model.predict(img)\n",
    "    pred_idx = np.argmax(preds)\n",
    "    confidence = np.max(preds) * 100\n",
    "    label = class_names[pred_idx] if class_names else f\"Class_{pred_idx}\"\n",
    "    return label, confidence\n",
    "\n",
    "# -----------------------------\n",
    "# 4ï¸âƒ£ GPT Assistant Function\n",
    "# -----------------------------\n",
    "def get_gpt_response(pred_label, confidence):\n",
    "    prompt = f\"\"\"\n",
    "    You are an agricultural expert AI.\n",
    "    The image analysis model detected: {pred_label} with {confidence:.2f}% confidence.\n",
    "\n",
    "    Please provide:\n",
    "    1. The name of the plant and disease\n",
    "    2. A short description of this disease\n",
    "    3. The main causes\n",
    "    4. Organic and chemical remedies\n",
    "    5. Preventive measures for farmers\n",
    "    Make the tone friendly and concise.\n",
    "    \"\"\"\n",
    "\n",
    "    response = openai.ChatCompletion.create(\n",
    "        model=\"gpt-4o\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a helpful agricultural assistant.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        temperature=0.7,\n",
    "        max_tokens=400\n",
    "    )\n",
    "    return response.choices[0].message['content'].strip()\n",
    "\n",
    "# -----------------------------\n",
    "# 5ï¸âƒ£ Full AI Assistant Flow\n",
    "# -----------------------------\n",
    "def run_plant_ai_assistant(image_path):\n",
    "    print(\"ğŸ§  Analyzing image...\")\n",
    "    pred_label, conf = predict_plant_disease(image_path)\n",
    "    print(f\"âœ… Predicted: {pred_label} ({conf:.2f}% confidence)\")\n",
    "    print(\"ğŸ’¬ Asking GPT-4.0 for expert advice...\\n\")\n",
    "\n",
    "    advice = get_gpt_response(pred_label, conf)\n",
    "    print(\"ğŸŒ¿ GPT-4.0 Plant Disease Assistant says:\\n\")\n",
    "    print(advice)\n",
    "\n",
    "# -----------------------------\n",
    "# 6ï¸âƒ£ Example Run\n",
    "# -----------------------------\n",
    "image_path = \"test_images/leaf.jpg\"  # change to your image\n",
    "run_plant_ai_assistant(image_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "1adcea4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "TensorFlow version: 2.20.0\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"TensorFlow version:\", tf.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "eff41cc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.12.0\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(keras.__version__)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60780b1a",
   "metadata": {},
   "outputs": [],
   "source": [
    "OLD_MODEL_PATH = \"C:/Users/Ayush/ath_codee/1_CNN_MODEL_code/efficientnetb0_plant_species.h5\"\n",
    "\n",
    "base_model = load_model(OLD_MODEL_PATH)\n",
    "\n",
    "# Inspect old model summary (optional)\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7ab918fb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">80</span>)             â”‚       <span style=\"color: #00af00; text-decoration-color: #00af00\">102,480</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m4,049,571\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m80\u001b[0m)             â”‚       \u001b[38;5;34m102,480\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,152,053</span> (15.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,152,053\u001b[0m (15.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,453,440</span> (5.54 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,453,440\u001b[0m (5.54 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2,698,611</span> (10.29 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m2,698,611\u001b[0m (10.29 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">2</span> (12.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m2\u001b[0m (12.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 validated image filenames belonging to 25 classes.\n",
      "Found 25 validated image filenames belonging to 24 classes.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = \"C:/Users/Ayush/ath_codee/new_custom_cleaned_dataset\"\n",
    "OLD_MODEL_PATH = \"efficientnetb0_plant_species.h5\"\n",
    "NEW_MODEL_PATH = \"efficientnetb0_plant_species_finetuned.h5\"\n",
    "IMG_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 10\n",
    "\n",
    "# Load old model WITHOUT top layer for architecture adjustment\n",
    "base_model = load_model(OLD_MODEL_PATH)\n",
    "\n",
    "# Inspect old model summary (optional)\n",
    "base_model.summary()\n",
    "\n",
    "# Extract existing layers except the last Dense layer output\n",
    "x = base_model.layers[-2].output  # Assuming last layer is Dense classifier\n",
    "\n",
    "# Create DataFrame of filenames and corresponding labels from cleaned dataset\n",
    "images = []\n",
    "labels = []\n",
    "\n",
    "for folder in os.listdir(DATASET_DIR):\n",
    "    folder_path = os.path.join(DATASET_DIR, folder)\n",
    "    if not os.path.isdir(folder_path):\n",
    "        continue\n",
    "    for fname in os.listdir(folder_path):\n",
    "        if fname.lower().endswith(('.png', '.jpg', '.jpeg')):\n",
    "            images.append(os.path.join(folder_path, fname))\n",
    "            labels.append(folder)\n",
    "\n",
    "df = pd.DataFrame({'filename': images, 'label': labels})\n",
    "\n",
    "# Filter classes with at least two samples for stratify splitting\n",
    "counts = df['label'].value_counts()\n",
    "valid_labels = counts[counts >= 2].index\n",
    "df = df[df['label'].isin(valid_labels)]\n",
    "\n",
    "# Train/Test split stratified by label\n",
    "num_classes = len(df['label'].unique())\n",
    "total_samples = len(df)\n",
    "test_size = max(0.2, num_classes / total_samples)  # dynamic test size based on num classes\n",
    "\n",
    "train_df, test_df = train_test_split(\n",
    "    df,\n",
    "    test_size=test_size,\n",
    "    stratify=df['label'],\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "# Data generators with preprocessing\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    rotation_range=20,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "test_datagen = ImageDataGenerator(preprocessing_function=tf.keras.applications.efficientnet.preprocess_input)\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    dataframe=train_df,\n",
    "    x_col='filename', y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe=test_df,\n",
    "    x_col='filename', y_col='label',\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)\n",
    "# Assuming `x` is the last layer before output:\n",
    "new_outputs = tf.keras.layers.Dense(num_classes, activation='softmax')(x)\n",
    "model = tf.keras.Model(inputs=base_model.input, outputs=new_outputs)\n",
    "\n",
    "# Adjust output layer to match new number of classes\n",
    "new_outputs = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Construct new Model with old input and new output\n",
    "model = Model(inputs=base_model.input, outputs=new_outputs)\n",
    "\n",
    "# Optionally freeze base layers for initial training\n",
    "for layer in base_model.layers[:-1]:\n",
    "    layer.trainable = False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352df242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train generator class indices:\", train_generator.class_indices)\n",
    "print(\"Number of classes in train generator:\", len(train_generator.class_indices))\n",
    "print(\"Model output layer units:\", model.output_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92992a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate found and removed: C:/Users/Ayush/ath_codee/cleaned_dataset_3\\Lemon grass Rust\\3.jpg\n",
      "Class distribution after cleaning:\n",
      "Aloe Leaf spot: 3\n",
      "Aloe Soft rot: 2\n",
      "Arali Leaf blight: 3\n",
      "Bamboo Rust: 2\n",
      "banana Panama wilt: 2\n",
      "banana Sigatoka: 3\n",
      "Chilli Leaf curl virus: 2\n",
      "Curry leaf black spot: 2\n",
      "Ginger Leaf spot: 3\n",
      "Guava Anthracnose: 0\n",
      "Guava Rust: 3\n",
      "Hibiscus Leaf spot: 2\n",
      "Hibiscus Rust: 4\n",
      "Jasmine Leaf spot: 2\n",
      "Jasmine Rust: 3\n",
      "Lemon grass Helminthosporium spot: 3\n",
      "Lemon grass Leaf blight: 3\n",
      "Lemon grass Rust: 2\n",
      "Mint Leaf Blight: 3\n",
      "Mint Rust: 4\n",
      "Neem Powdery mildew: 1\n",
      "Pepper Phytophthora foot rot: 2\n",
      "Powdery mildew: 5\n",
      "Rose Black spot: 2\n",
      "Sooty mold: 4\n",
      "Tumeric Leaf blotch: 4\n",
      "Tumeric Leaf spot: 3\n",
      "Found 71 images belonging to 27 classes.\n",
      "Found 1 images belonging to 27 classes.\n",
      "Number of classes: 27\n",
      "Training samples: 71\n",
      "Validation samples: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "DATASET_PATH = \"C:/Users/Ayush/ath_codee/2_custom_plant_disease_dataset__\"\n",
    "CLEANED_DATASET_PATH = \"C:/Users/Ayush/ath_codee/cleaned_dataset_3\"\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Step 1: Remove corrupted images and copy valid images to cleaned_dataset\n",
    "def remove_corrupted_and_copy(src_path, dest_path):\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "    for class_folder in os.listdir(src_path):\n",
    "        class_path = os.path.join(src_path, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        dest_class_path = os.path.join(dest_path, class_folder)\n",
    "        os.makedirs(dest_class_path, exist_ok=True)\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img.verify()  # Verify image valid\n",
    "                # Reopen and convert to RGB then save resized copy\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = img.resize(IMAGE_SIZE)\n",
    "                img.save(os.path.join(dest_class_path, img_file))\n",
    "            except (IOError, SyntaxError) as e:\n",
    "                print(f\"Removing corrupted image: {img_path}\")\n",
    "\n",
    "# Step 2: Remove duplicate images (by hash) in cleaned_dataset\n",
    "def remove_duplicates(dataset_path):\n",
    "    hashes = set()\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, class_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            if file_hash in hashes:\n",
    "                print(f\"Duplicate found and removed: {img_path}\")\n",
    "                os.remove(img_path)\n",
    "            else:\n",
    "                hashes.add(file_hash)\n",
    "\n",
    "# Step 6: Check class imbalance and output class counts\n",
    "def print_class_distribution(dataset_path):\n",
    "    class_counts = {}\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            count = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            class_counts[class_folder] = count\n",
    "    print(\"Class distribution after cleaning:\")\n",
    "    for cls, cnt in class_counts.items():\n",
    "        print(f\"{cls}: {cnt}\")\n",
    "\n",
    "# Run cleaning steps\n",
    "remove_corrupted_and_copy(DATASET_PATH, CLEANED_DATASET_PATH)\n",
    "remove_duplicates(CLEANED_DATASET_PATH)\n",
    "print_class_distribution(CLEANED_DATASET_PATH)\n",
    "\n",
    "# Step 4 & 5: Data normalization and augmentation with ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    CLEANED_DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    CLEANED_DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# The train_generator and validation_generator now have normalized and augmented batches ready for model training.\n",
    "\n",
    "print(f\"Number of classes: {len(train_generator.class_indices)}\")\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {validation_generator.samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0c542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split completed and data saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3'  # Path to your cleaned dataset folder\n",
    "TRAIN_DIR = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3_train'\n",
    "TEST_DIR = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3_test'\n",
    "TEST_SIZE = 0.15  # 15% for testing\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "# For each class folder in the dataset\n",
    "for class_folder in os.listdir(DATASET_DIR):\n",
    "    class_path = os.path.join(DATASET_DIR, class_folder)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calculate split index\n",
    "    split_idx = int(len(images) * (1 - TEST_SIZE))\n",
    "    train_images = images[:split_idx]\n",
    "    test_images = images[split_idx:]\n",
    "\n",
    "    # Create class folders in train and test directories\n",
    "    train_class_path = os.path.join(TRAIN_DIR, class_folder)\n",
    "    test_class_path = os.path.join(TEST_DIR, class_folder)\n",
    "    os.makedirs(train_class_path, exist_ok=True)\n",
    "    os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "    # Move images to train folder\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(class_path, img), train_class_path)\n",
    "\n",
    "    # Move images to test folder\n",
    "    for img in test_images:\n",
    "        shutil.copy(os.path.join(class_path, img), test_class_path)\n",
    "\n",
    "print(\"Train-test split completed and data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5b400bdd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 27 classes.\n",
      "Found 1 images belonging to 27 classes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Input 0 of layer \"global_average_pooling2d\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 80)",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[5]\u001b[39m\u001b[32m, line 55\u001b[39m\n\u001b[32m     53\u001b[39m inputs = tf.keras.Input(shape=(*IMAGE_SIZE, \u001b[32m3\u001b[39m))\n\u001b[32m     54\u001b[39m x = base_model(inputs, training=\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[32m---> \u001b[39m\u001b[32m55\u001b[39m x = \u001b[43mGlobalAveragePooling2D\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     56\u001b[39m x = Dropout(\u001b[32m0.3\u001b[39m)(x)\n\u001b[32m     57\u001b[39m outputs = Dense(num_classes, activation=\u001b[33m'\u001b[39m\u001b[33msoftmax\u001b[39m\u001b[33m'\u001b[39m)(x)  \u001b[38;5;66;03m# Softmax for multi-class classification\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\site-packages\\keras\\src\\layers\\input_spec.py:186\u001b[39m, in \u001b[36massert_input_compatibility\u001b[39m\u001b[34m(input_spec, inputs, layer_name)\u001b[39m\n\u001b[32m    184\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m spec.allow_last_axis_squeeze:\n\u001b[32m    185\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ndim != spec.ndim:\n\u001b[32m--> \u001b[39m\u001b[32m186\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m    187\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m'\u001b[39m\u001b[33mInput \u001b[39m\u001b[38;5;132;01m{\u001b[39;00minput_index\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m of layer \u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mlayer_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\u001b[33m \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m    188\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mis incompatible with the layer: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    189\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mexpected ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mspec.ndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m, found ndim=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mndim\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    190\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mFull shape received: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mshape\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m    191\u001b[39m         )\n\u001b[32m    192\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m spec.max_ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    193\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m ndim \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;129;01mand\u001b[39;00m ndim > spec.max_ndim:\n",
      "\u001b[31mValueError\u001b[39m: Input 0 of layer \"global_average_pooling2d\" is incompatible with the layer: expected ndim=4, found ndim=2. Full shape received: (None, 80)"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "\n",
    "# Set dataset path and image parameters\n",
    "DATASET_PATH = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3'\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "EPOCHS = 20\n",
    "\n",
    "# Data augmentation and preprocessing with EfficientNet preprocessing\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "# Train and validation generators with categorical class_mode for multi-class classification\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',  # Important for one-hot encoded labels\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "num_classes = len(train_generator.class_indices)  # Number of plant species + disease classes\n",
    "\n",
    "# Load pre-trained EfficientNetB0 base model without the top classification layer\n",
    "base_model = tf.keras.models.load_model('C:/Users/Ayush/ath_codee/1_CNN_MODEL_code/efficientnetb0_plant_species.h5')\n",
    "\n",
    "# Construct new model with modified output layer for your classes\n",
    "inputs = tf.keras.Input(shape=(*IMAGE_SIZE, 3))\n",
    "x = base_model(inputs, training=False)\n",
    "x = GlobalAveragePooling2D()(x)\n",
    "x = Dropout(0.3)(x)\n",
    "outputs = Dense(num_classes, activation='softmax')(x)  # Softmax for multi-class classification\n",
    "\n",
    "model = Model(inputs, outputs)\n",
    "\n",
    "# Compile model with categorical crossentropy loss and accuracy metric\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Define callbacks for early stopping, learning rate reduction and best model saving\n",
    "callbacks = [\n",
    "    EarlyStopping(monitor='val_loss', patience=5, restore_best_weights=True),\n",
    "    ReduceLROnPlateau(monitor='val_loss', factor=0.2, patience=3, verbose=1),\n",
    "    ModelCheckpoint('best_model.h5', monitor='val_loss', save_best_only=True)\n",
    "]\n",
    "\n",
    "# Train the model\n",
    "history = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=EPOCHS,\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "# Optional fine-tuning: unfreeze some layers and recompile with lowered learning rate, then train\n",
    "base_model.trainable = True\n",
    "fine_tune_at = len(base_model.layers) - 20  # Unfreeze last 20 layers\n",
    "\n",
    "for layer in base_model.layers[:fine_tune_at]:\n",
    "    layer.trainable = False\n",
    "\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(1e-5),\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "fine_tune_epochs = 10\n",
    "total_epochs = EPOCHS + fine_tune_epochs\n",
    "\n",
    "history_fine = model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=total_epochs,\n",
    "    initial_epoch=history.epoch[-1],\n",
    "    callbacks=callbacks\n",
    ")\n",
    "\n",
    "print(\"Training and fine-tuning complete.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c263db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 27 classes.\n",
      "Found 1 images belonging to 27 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.0563 - loss: 3.4461 - val_accuracy: 0.0000e+00 - val_loss: 3.3821\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 432ms/step - accuracy: 0.0986 - loss: 3.1252 - val_accuracy: 0.0000e+00 - val_loss: 3.1036\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 652ms/step - accuracy: 0.2817 - loss: 2.8682 - val_accuracy: 0.0000e+00 - val_loss: 3.0407\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 449ms/step - accuracy: 0.3803 - loss: 2.6699 - val_accuracy: 0.0000e+00 - val_loss: 3.1214\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 470ms/step - accuracy: 0.4648 - loss: 2.5379 - val_accuracy: 0.0000e+00 - val_loss: 2.9034\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 700ms/step - accuracy: 0.5775 - loss: 2.3322 - val_accuracy: 1.0000 - val_loss: 2.7180\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 769ms/step - accuracy: 0.6901 - loss: 2.1446 - val_accuracy: 0.0000e+00 - val_loss: 2.6041\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 496ms/step - accuracy: 0.7887 - loss: 1.9784 - val_accuracy: 0.0000e+00 - val_loss: 2.4472\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 768ms/step - accuracy: 0.8451 - loss: 1.8024 - val_accuracy: 0.0000e+00 - val_loss: 2.7216\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 774ms/step - accuracy: 0.8451 - loss: 1.6232 - val_accuracy: 0.0000e+00 - val_loss: 2.4422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1694f19c440>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Constants\n",
    "DATASET_PATH = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3'\n",
    "OLD_MODEL_PATH = 'C:/Users/Ayush/ath_codee/1_CNN_MODEL_code/efficientnetb0_plant_species.h5'\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "EPOCHS = 10\n",
    "\n",
    "# Load your existing trained model (with included pooling and classifier)\n",
    "base_model = load_model(OLD_MODEL_PATH)\n",
    "\n",
    "# Create data generators\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get correct number of classes dynamically\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Remove last Dense layer from base_model and get second last layer output\n",
    "x = base_model.layers[-3].output  # Dropout layer output (before Dense)\n",
    "\n",
    "# Add new Dense layer for new number of classes\n",
    "new_output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create new model\n",
    "model = Model(inputs=base_model.input, outputs=new_output)\n",
    "\n",
    "# Freeze base layers initially\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "3f23d40b",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[13]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnew_efficientnetb0_disease_detector\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msave_format\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mtf\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel saved in TensorFlow SavedModel format in folder \u001b[39m\u001b[33m'\u001b[39m\u001b[33mnew_efficientnetb0_disease_detector\u001b[39m\u001b[33m'\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\site-packages\\keras\\src\\saving\\saving_api.py:69\u001b[39m, in \u001b[36msave_model\u001b[39m\u001b[34m(model, filepath, overwrite, zipped, **kwargs)\u001b[39m\n\u001b[32m     62\u001b[39m         logging.warning(\n\u001b[32m     63\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe `save_format` argument is deprecated in Keras 3. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     64\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mWe recommend removing this argument as it can be inferred \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     65\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mfrom the file path. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     66\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived: save_format=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     67\u001b[39m         )\n\u001b[32m     68\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m---> \u001b[39m\u001b[32m69\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     70\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mThe `save_format` argument is deprecated in Keras 3. \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     71\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33mPlease remove this argument and pass a file path with \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     72\u001b[39m             \u001b[33m\"\u001b[39m\u001b[33meither `.keras` or `.h5` extension.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     73\u001b[39m             \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mReceived: save_format=\u001b[39m\u001b[38;5;132;01m{\u001b[39;00msave_format\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     74\u001b[39m         )\n\u001b[32m     75\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m kwargs:\n\u001b[32m     76\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m     77\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mThe following argument(s) are not supported: \u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m     78\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mlist\u001b[39m(kwargs.keys())\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m\n\u001b[32m     79\u001b[39m     )\n",
      "\u001b[31mValueError\u001b[39m: The `save_format` argument is deprecated in Keras 3. Please remove this argument and pass a file path with either `.keras` or `.h5` extension.Received: save_format=tf"
     ]
    }
   ],
   "source": [
    "model.save('new_efficientnetb0_disease_detector', save_format='tf')\n",
    "print(\"Model saved in TensorFlow SavedModel format in folder 'new_efficientnetb0_disease_detector'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "06df4eaa",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "cannot pickle 'module' object",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[12]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mnew_efficientnetb0_disease_detector.h5\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mModel saved as new_efficientnetb0_disease_detector.h5\u001b[39m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\site-packages\\keras\\src\\utils\\traceback_utils.py:122\u001b[39m, in \u001b[36mfilter_traceback.<locals>.error_handler\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    119\u001b[39m     filtered_tb = _process_traceback_frames(e.__traceback__)\n\u001b[32m    120\u001b[39m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[32m    121\u001b[39m     \u001b[38;5;66;03m# `keras.config.disable_traceback_filtering()`\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m122\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m e.with_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    123\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    124\u001b[39m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:197\u001b[39m, in \u001b[36m_deepcopy_list\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    195\u001b[39m append = y.append\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:163\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    161\u001b[39m                 y = x\n\u001b[32m    162\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:260\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    262\u001b[39m         y.__setstate__(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:163\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    161\u001b[39m                 y = x\n\u001b[32m    162\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:260\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    262\u001b[39m         y.__setstate__(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:202\u001b[39m, in \u001b[36m_deepcopy_tuple\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    201\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34m_deepcopy_tuple\u001b[39m(x, memo, deepcopy=deepcopy):\n\u001b[32m--> \u001b[39m\u001b[32m202\u001b[39m     y = [deepcopy(a, memo) \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x]\n\u001b[32m    203\u001b[39m     \u001b[38;5;66;03m# We're not going to put the tuple in the memo, but it's still important we\u001b[39;00m\n\u001b[32m    204\u001b[39m     \u001b[38;5;66;03m# check for it, in case the tuple contains recursive mutable structures.\u001b[39;00m\n\u001b[32m    205\u001b[39m     \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:197\u001b[39m, in \u001b[36m_deepcopy_list\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    195\u001b[39m append = y.append\n\u001b[32m    196\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m a \u001b[38;5;129;01min\u001b[39;00m x:\n\u001b[32m--> \u001b[39m\u001b[32m197\u001b[39m     append(\u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m)\n\u001b[32m    198\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:163\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    161\u001b[39m                 y = x\n\u001b[32m    162\u001b[39m             \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m163\u001b[39m                 y = \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    165\u001b[39m \u001b[38;5;66;03m# If is its own copy, don't memoize.\u001b[39;00m\n\u001b[32m    166\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m y \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m x:\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:260\u001b[39m, in \u001b[36m_reconstruct\u001b[39m\u001b[34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[39m\n\u001b[32m    258\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m state \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m    259\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m deep:\n\u001b[32m--> \u001b[39m\u001b[32m260\u001b[39m         state = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mstate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    261\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(y, \u001b[33m'\u001b[39m\u001b[33m__setstate__\u001b[39m\u001b[33m'\u001b[39m):\n\u001b[32m    262\u001b[39m         y.__setstate__(state)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:137\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    135\u001b[39m copier = _deepcopy_dispatch.get(\u001b[38;5;28mcls\u001b[39m)\n\u001b[32m    136\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copier \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m137\u001b[39m     y = \u001b[43mcopier\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    138\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    139\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28missubclass\u001b[39m(\u001b[38;5;28mcls\u001b[39m, \u001b[38;5;28mtype\u001b[39m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:222\u001b[39m, in \u001b[36m_deepcopy_dict\u001b[39m\u001b[34m(x, memo, deepcopy)\u001b[39m\n\u001b[32m    220\u001b[39m memo[\u001b[38;5;28mid\u001b[39m(x)] = y\n\u001b[32m    221\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m key, value \u001b[38;5;129;01min\u001b[39;00m x.items():\n\u001b[32m--> \u001b[39m\u001b[32m222\u001b[39m     y[deepcopy(key, memo)] = \u001b[43mdeepcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvalue\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmemo\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    223\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\Ayush\\ath_codee\\venv\\Lib\\copy.py:152\u001b[39m, in \u001b[36mdeepcopy\u001b[39m\u001b[34m(x, memo, _nil)\u001b[39m\n\u001b[32m    150\u001b[39m reductor = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__reduce_ex__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n\u001b[32m    151\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m reductor \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m152\u001b[39m     rv = \u001b[43mreductor\u001b[49m\u001b[43m(\u001b[49m\u001b[32;43m4\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[32m    153\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m    154\u001b[39m     reductor = \u001b[38;5;28mgetattr\u001b[39m(x, \u001b[33m\"\u001b[39m\u001b[33m__reduce__\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m)\n",
      "\u001b[31mTypeError\u001b[39m: cannot pickle 'module' object"
     ]
    }
   ],
   "source": [
    "model.save('new_efficientnetb0_disease_detector.h5')\n",
    "print(\"Model saved as new_efficientnetb0_disease_detector.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a2758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as new_efficientnetb0_disease_detector.keras\n"
     ]
    }
   ],
   "source": [
    "model.save('new_efficientnetb0_disease_detector.keras')\n",
    "print(\"Model saved as new_efficientnetb0_disease_detector.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d33997a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">34,587</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m4,049,571\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)             â”‚        \u001b[38;5;34m34,587\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,153,334</span> (15.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,153,334\u001b[0m (15.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,587</span> (135.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,587\u001b[0m (135.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,176</span> (270.22 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m69,176\u001b[0m (270.22 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Replace the path below with your actual model file path\n",
    "model_path = 'new_efficientnetb0_disease_detector.keras'  # or .h5 if using legacy format\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
