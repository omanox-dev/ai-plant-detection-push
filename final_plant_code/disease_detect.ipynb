{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "2ed92c56",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "import random\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.applications import EfficientNetB0\n",
    "from tensorflow.keras.layers import Dense, GlobalAveragePooling2D, Dropout\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "import cv2\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras import layers, models\n",
    "from tensorflow.keras.layers import BatchNormalization\n",
    "#import openai\n",
    "#import json\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "3591f1a8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Removed 0 corrupted images.\n",
      "âœ… Removed 0 duplicate images.\n",
      "âœ… All images resized to (224, 224) and saved in 'C:/Users/Ayush/ath_codee/new_custom_cleaned_dataset'.\n",
      "âœ… Data normalization (rescaling 1./255) will be applied during training.\n",
      "Found 72 images belonging to 27 classes.\n",
      "Found 0 images belonging to 27 classes.\n",
      "âœ… Data augmentation pipeline ready (rotation, flip, zoom, brightness, etc.)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# ----------------------------\n",
    "# Configuration\n",
    "# ----------------------------\n",
    "SOURCE_DIR = \"C:/Users/Ayush/ath_codee/2_custom_plant_disease_dataset__\"            # path to your raw dataset\n",
    "CLEANED_DIR = \"C:/Users/Ayush/ath_codee/new_custom_cleaned_dataset\"   # output folder for cleaned dataset\n",
    "IMG_SIZE = (224, 224)                    # target size for CNN\n",
    "os.makedirs(CLEANED_DIR, exist_ok=True)\n",
    "\n",
    "# ----------------------------\n",
    "# 1ï¸âƒ£ Remove Corrupted Images\n",
    "# ----------------------------\n",
    "def remove_corrupted_images(source_dir):\n",
    "    removed = 0\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            try:\n",
    "                img = Image.open(file_path)\n",
    "                img.verify()  # verifies if image is readable\n",
    "            except Exception:\n",
    "                print(f\"âŒ Removing corrupted image: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "                removed += 1\n",
    "    print(f\"âœ… Removed {removed} corrupted images.\")\n",
    "\n",
    "remove_corrupted_images(SOURCE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 2ï¸âƒ£ Remove Duplicate Images\n",
    "# ----------------------------\n",
    "def remove_duplicate_images(source_dir):\n",
    "    seen_hashes = set()\n",
    "    duplicates = 0\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for file in os.listdir(folder_path):\n",
    "            file_path = os.path.join(folder_path, file)\n",
    "            with open(file_path, \"rb\") as f:\n",
    "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            if file_hash in seen_hashes:\n",
    "                print(f\"ğŸ—‘ Removing duplicate image: {file_path}\")\n",
    "                os.remove(file_path)\n",
    "                duplicates += 1\n",
    "            else:\n",
    "                seen_hashes.add(file_hash)\n",
    "    print(f\"âœ… Removed {duplicates} duplicate images.\")\n",
    "\n",
    "remove_duplicate_images(SOURCE_DIR)\n",
    "\n",
    "# ----------------------------\n",
    "# 3ï¸âƒ£ Resize Images\n",
    "# ----------------------------\n",
    "def resize_images(source_dir, output_dir, target_size=(224, 224)):\n",
    "    for folder in os.listdir(source_dir):\n",
    "        folder_path = os.path.join(source_dir, folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        output_folder = os.path.join(output_dir, folder)\n",
    "        os.makedirs(output_folder, exist_ok=True)\n",
    "        for file in os.listdir(folder_path):\n",
    "            src_path = os.path.join(folder_path, file)\n",
    "            dst_path = os.path.join(output_folder, file)\n",
    "            try:\n",
    "                img = Image.open(src_path).convert(\"RGB\")\n",
    "                img = img.resize(target_size)\n",
    "                img.save(dst_path)\n",
    "            except Exception as e:\n",
    "                print(f\"âš ï¸ Error resizing {file}: {e}\")\n",
    "    print(f\"âœ… All images resized to {target_size} and saved in '{output_dir}'.\")\n",
    "\n",
    "resize_images(SOURCE_DIR, CLEANED_DIR, IMG_SIZE)\n",
    "\n",
    "# ----------------------------\n",
    "# 4ï¸âƒ£ Data Normalization (for model input)\n",
    "# ----------------------------\n",
    "# Note: We'll apply normalization dynamically using ImageDataGenerator below.\n",
    "# This step ensures pixel values are scaled between 0 and 1.\n",
    "\n",
    "datagen = ImageDataGenerator(rescale=1./255)\n",
    "print(\"âœ… Data normalization (rescaling 1./255) will be applied during training.\")\n",
    "\n",
    "# ----------------------------\n",
    "# 5ï¸âƒ£ Data Augmentation\n",
    "# ----------------------------\n",
    "# Youâ€™ll apply this dynamically during training for better generalization.\n",
    "# Example augmentations for plant leaf datasets:\n",
    "\n",
    "aug_datagen = ImageDataGenerator(\n",
    "    rescale=1./255,\n",
    "    rotation_range=25,\n",
    "    width_shift_range=0.15,\n",
    "    height_shift_range=0.15,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    fill_mode='nearest',\n",
    "    validation_split=0.15  # 85% training, 15% validation\n",
    ")\n",
    "\n",
    "# Example usage â€” generator creation\n",
    "train_gen = aug_datagen.flow_from_directory(\n",
    "    CLEANED_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='training'\n",
    ")\n",
    "\n",
    "val_gen = aug_datagen.flow_from_directory(\n",
    "    CLEANED_DIR,\n",
    "    target_size=IMG_SIZE,\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    subset='validation'\n",
    ")\n",
    "\n",
    "print(\"âœ… Data augmentation pipeline ready (rotation, flip, zoom, brightness, etc.)\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "81ce2cf9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Aloe Leaf spot: 2 train, 0 val, 1 test\n",
      "âœ… Aloe Soft rot: 1 train, 0 val, 1 test\n",
      "âœ… Arali Leaf blight: 2 train, 0 val, 1 test\n",
      "âœ… Bamboo Rust: 1 train, 0 val, 1 test\n",
      "âœ… banana Panama wilt: 1 train, 0 val, 1 test\n",
      "âœ… banana Sigatoka: 2 train, 0 val, 1 test\n",
      "âœ… Chilli Leaf curl virus: 1 train, 0 val, 1 test\n",
      "âœ… Curry leaf black spot: 1 train, 0 val, 1 test\n",
      "âœ… Ginger Leaf spot: 2 train, 0 val, 1 test\n",
      "âœ… Guava Anthracnose: 0 train, 0 val, 0 test\n",
      "âœ… Guava Rust: 2 train, 0 val, 1 test\n",
      "âœ… Hibiscus Leaf spot: 1 train, 0 val, 1 test\n",
      "âœ… Hibiscus Rust: 3 train, 0 val, 1 test\n",
      "âœ… Jasmine Leaf spot: 1 train, 0 val, 1 test\n",
      "âœ… Jasmine Rust: 2 train, 0 val, 1 test\n",
      "âœ… Lemon grass Helminthosporium spot: 2 train, 0 val, 1 test\n",
      "âœ… Lemon grass Leaf blight: 2 train, 0 val, 1 test\n",
      "âœ… Lemon grass Rust: 1 train, 0 val, 1 test\n",
      "âœ… Mint Leaf Blight: 2 train, 0 val, 1 test\n",
      "âœ… Mint Rust: 3 train, 0 val, 1 test\n",
      "âœ… Neem Powdery mildew: 0 train, 0 val, 1 test\n",
      "âœ… Pepper Phytophthora foot rot: 1 train, 0 val, 1 test\n",
      "âœ… Powdery mildew: 4 train, 0 val, 1 test\n",
      "âœ… Rose Black spot: 1 train, 0 val, 1 test\n",
      "âœ… Sooty mold: 3 train, 0 val, 1 test\n",
      "âœ… Tumeric Leaf blotch: 3 train, 0 val, 1 test\n",
      "âœ… Tumeric Leaf spot: 2 train, 0 val, 1 test\n",
      "\n",
      "ğŸ¯ Data successfully split and saved to 'custom_split_dataset/'\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Paths\n",
    "SOURCE_DIR = \"C:/Users/Ayush/ath_codee/2_custom_plant_disease_dataset__\"\n",
    "DEST_DIR = \"C:/Users/Ayush/ath_codee/new_custom_split_dataset\"\n",
    "\n",
    "# Ratios\n",
    "TRAIN_SPLIT = 0.8\n",
    "VAL_SPLIT = 0.1\n",
    "TEST_SPLIT = 0.1\n",
    "\n",
    "# Create destination folders\n",
    "for split in [\"train\", \"val\", \"test\"]:\n",
    "    os.makedirs(os.path.join(DEST_DIR, split), exist_ok=True)\n",
    "\n",
    "# Split each class folder\n",
    "for class_name in os.listdir(SOURCE_DIR):\n",
    "    class_path = os.path.join(SOURCE_DIR, class_name)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "    \n",
    "    # Destination class folders\n",
    "    train_folder = os.path.join(DEST_DIR, \"train\", class_name)\n",
    "    val_folder = os.path.join(DEST_DIR, \"val\", class_name)\n",
    "    test_folder = os.path.join(DEST_DIR, \"test\", class_name)\n",
    "    os.makedirs(train_folder, exist_ok=True)\n",
    "    os.makedirs(val_folder, exist_ok=True)\n",
    "    os.makedirs(test_folder, exist_ok=True)\n",
    "    \n",
    "    # Get image files\n",
    "    images = [f for f in os.listdir(class_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calculate split indices\n",
    "    total = len(images)\n",
    "    train_end = int(total * TRAIN_SPLIT)\n",
    "    val_end = train_end + int(total * VAL_SPLIT)\n",
    "\n",
    "    # Split lists\n",
    "    train_imgs = images[:train_end]\n",
    "    val_imgs = images[train_end:val_end]\n",
    "    test_imgs = images[val_end:]\n",
    "\n",
    "    # Copy images\n",
    "    for img in train_imgs:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(train_folder, img))\n",
    "    for img in val_imgs:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(val_folder, img))\n",
    "    for img in test_imgs:\n",
    "        shutil.copy(os.path.join(class_path, img), os.path.join(test_folder, img))\n",
    "    \n",
    "    print(f\"âœ… {class_name}: {len(train_imgs)} train, {len(val_imgs)} val, {len(test_imgs)} test\")\n",
    "\n",
    "print(\"\\nğŸ¯ Data successfully split and saved to 'custom_split_dataset/'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf6dddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dir = \"C:/Users/Ayush/ath_codee/them...........DATASETS!!/2_custom_cleaned_dataset/custom_split_dataset/train\"\n",
    "val_dir = \"C:/Users/Ayush/ath_codee/them...........DATASETS!!/2_custom_cleaned_dataset/custom_split_dataset/val\"\n",
    "test_dir = \"C:/Users/Ayush/ath_codee/them...........DATASETS!!/2_custom_cleaned_dataset/custom_split_dataset/test\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "9104562a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 46 images belonging to 27 classes.\n",
      "Found 0 images belonging to 27 classes.\n",
      "Found 26 images belonging to 27 classes.\n"
     ]
    }
   ],
   "source": [
    "train_gen = aug_datagen.flow_from_directory(\n",
    "    train_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "val_gen = aug_datagen.flow_from_directory(\n",
    "    val_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical'\n",
    ")\n",
    "\n",
    "test_gen = datagen.flow_from_directory(\n",
    "    test_dir,\n",
    "    target_size=(224, 224),\n",
    "    batch_size=32,\n",
    "    class_mode='categorical',\n",
    "    shuffle=False\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "f45d8709",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset split complete:\n",
      "Train dir: path_to_train_dir\n",
      "Validation dir: path_to_val_dir\n",
      "Test dir: path_to_test_dir\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "def split_dataset(\n",
    "    source_dir, train_dir, val_dir, test_dir,\n",
    "    train_split=0.7, val_split=0.15, test_split=0.15,\n",
    "    seed=42\n",
    "):\n",
    "    random.seed(seed)\n",
    "    # Create directories if not exists\n",
    "    for d in [train_dir, val_dir, test_dir]:\n",
    "        if not os.path.exists(d):\n",
    "            os.makedirs(d)\n",
    "\n",
    "    classes = [d for d in os.listdir(source_dir) if os.path.isdir(os.path.join(source_dir, d))]\n",
    "\n",
    "    for cls in classes:\n",
    "        cls_path = os.path.join(source_dir, cls)\n",
    "        images = [f for f in os.listdir(cls_path) if f.lower().endswith(('.jpg', '.jpeg', '.png'))]\n",
    "        random.shuffle(images)\n",
    "\n",
    "        n_total = len(images)\n",
    "        n_train = int(n_total * train_split)\n",
    "        n_val = int(n_total * val_split)\n",
    "        n_test = n_total - n_train - n_val  # rest\n",
    "\n",
    "        # Make class-wise directories in train/val/test\n",
    "        for split_dir in [train_dir, val_dir, test_dir]:\n",
    "            os.makedirs(os.path.join(split_dir, cls), exist_ok=True)\n",
    "\n",
    "        # Copy files to respective dirs\n",
    "        for i, img in enumerate(images):\n",
    "            src_path = os.path.join(cls_path, img)\n",
    "            if i < n_train:\n",
    "                dst_path = os.path.join(train_dir, cls, img)\n",
    "            elif i < (n_train + n_val):\n",
    "                dst_path = os.path.join(val_dir, cls, img)\n",
    "            else:\n",
    "                dst_path = os.path.join(test_dir, cls, img)\n",
    "            shutil.copy2(src_path, dst_path)\n",
    "\n",
    "    print(f\"Dataset split complete:\")\n",
    "    print(f\"Train dir: {train_dir}\")\n",
    "    print(f\"Validation dir: {val_dir}\")\n",
    "    print(f\"Test dir: {test_dir}\")\n",
    "\n",
    "# Example usage:\n",
    "source_dataset_dir = 'C:/Users/Ayush/ath_codee/2_custom_plant_disease_dataset__'\n",
    "train_directory = 'path_to_train_dir'\n",
    "val_directory = 'path_to_val_dir'\n",
    "test_directory = 'path_to_test_dir'\n",
    "\n",
    "split_dataset(source_dataset_dir, train_directory, val_directory, test_directory)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "352df242",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Train generator class indices:\", train_generator.class_indices)\n",
    "print(\"Number of classes in train generator:\", len(train_generator.class_indices))\n",
    "print(\"Model output layer units:\", model.output_shape[-1])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "92992a02",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Duplicate found and removed: C:/Users/Ayush/ath_codee/cleaned_dataset_3\\Lemon grass Rust\\3.jpg\n",
      "Class distribution after cleaning:\n",
      "Aloe Leaf spot: 3\n",
      "Aloe Soft rot: 2\n",
      "Arali Leaf blight: 3\n",
      "Bamboo Rust: 2\n",
      "banana Panama wilt: 2\n",
      "banana Sigatoka: 3\n",
      "Chilli Leaf curl virus: 2\n",
      "Curry leaf black spot: 2\n",
      "Ginger Leaf spot: 3\n",
      "Guava Anthracnose: 0\n",
      "Guava Rust: 3\n",
      "Hibiscus Leaf spot: 2\n",
      "Hibiscus Rust: 4\n",
      "Jasmine Leaf spot: 2\n",
      "Jasmine Rust: 3\n",
      "Lemon grass Helminthosporium spot: 3\n",
      "Lemon grass Leaf blight: 3\n",
      "Lemon grass Rust: 2\n",
      "Mint Leaf Blight: 3\n",
      "Mint Rust: 4\n",
      "Neem Powdery mildew: 1\n",
      "Pepper Phytophthora foot rot: 2\n",
      "Powdery mildew: 5\n",
      "Rose Black spot: 2\n",
      "Sooty mold: 4\n",
      "Tumeric Leaf blotch: 4\n",
      "Tumeric Leaf spot: 3\n",
      "Found 71 images belonging to 27 classes.\n",
      "Found 1 images belonging to 27 classes.\n",
      "Number of classes: 27\n",
      "Training samples: 71\n",
      "Validation samples: 1\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import hashlib\n",
    "from PIL import Image\n",
    "import numpy as np\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import tensorflow as tf\n",
    "\n",
    "# Constants\n",
    "DATASET_PATH = \"C:/Users/Ayush/ath_codee/2_custom_plant_disease_dataset__\"\n",
    "CLEANED_DATASET_PATH = \"C:/Users/Ayush/ath_codee/cleaned_dataset_3\"\n",
    "IMAGE_SIZE = (224, 224)\n",
    "BATCH_SIZE = 32\n",
    "\n",
    "# Step 1: Remove corrupted images and copy valid images to cleaned_dataset\n",
    "def remove_corrupted_and_copy(src_path, dest_path):\n",
    "    os.makedirs(dest_path, exist_ok=True)\n",
    "    for class_folder in os.listdir(src_path):\n",
    "        class_path = os.path.join(src_path, class_folder)\n",
    "        if not os.path.isdir(class_path):\n",
    "            continue\n",
    "        dest_class_path = os.path.join(dest_path, class_folder)\n",
    "        os.makedirs(dest_class_path, exist_ok=True)\n",
    "        for img_file in os.listdir(class_path):\n",
    "            img_path = os.path.join(class_path, img_file)\n",
    "            try:\n",
    "                img = Image.open(img_path)\n",
    "                img.verify()  # Verify image valid\n",
    "                # Reopen and convert to RGB then save resized copy\n",
    "                img = Image.open(img_path).convert(\"RGB\")\n",
    "                img = img.resize(IMAGE_SIZE)\n",
    "                img.save(os.path.join(dest_class_path, img_file))\n",
    "            except (IOError, SyntaxError) as e:\n",
    "                print(f\"Removing corrupted image: {img_path}\")\n",
    "\n",
    "# Step 2: Remove duplicate images (by hash) in cleaned_dataset\n",
    "def remove_duplicates(dataset_path):\n",
    "    hashes = set()\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        folder_path = os.path.join(dataset_path, class_folder)\n",
    "        if not os.path.isdir(folder_path):\n",
    "            continue\n",
    "        for img_file in os.listdir(folder_path):\n",
    "            img_path = os.path.join(folder_path, img_file)\n",
    "            with open(img_path, \"rb\") as f:\n",
    "                file_hash = hashlib.md5(f.read()).hexdigest()\n",
    "            if file_hash in hashes:\n",
    "                print(f\"Duplicate found and removed: {img_path}\")\n",
    "                os.remove(img_path)\n",
    "            else:\n",
    "                hashes.add(file_hash)\n",
    "\n",
    "# Step 6: Check class imbalance and output class counts\n",
    "def print_class_distribution(dataset_path):\n",
    "    class_counts = {}\n",
    "    for class_folder in os.listdir(dataset_path):\n",
    "        class_path = os.path.join(dataset_path, class_folder)\n",
    "        if os.path.isdir(class_path):\n",
    "            count = len([f for f in os.listdir(class_path) if f.lower().endswith(('.png', '.jpg', '.jpeg'))])\n",
    "            class_counts[class_folder] = count\n",
    "    print(\"Class distribution after cleaning:\")\n",
    "    for cls, cnt in class_counts.items():\n",
    "        print(f\"{cls}: {cnt}\")\n",
    "\n",
    "# Run cleaning steps\n",
    "remove_corrupted_and_copy(DATASET_PATH, CLEANED_DATASET_PATH)\n",
    "remove_duplicates(CLEANED_DATASET_PATH)\n",
    "print_class_distribution(CLEANED_DATASET_PATH)\n",
    "\n",
    "# Step 4 & 5: Data normalization and augmentation with ImageDataGenerator\n",
    "train_datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    rotation_range=30,\n",
    "    width_shift_range=0.1,\n",
    "    height_shift_range=0.1,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True,\n",
    "    brightness_range=[0.8, 1.2],\n",
    "    validation_split=0.2\n",
    ")\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(\n",
    "    CLEANED_DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = train_datagen.flow_from_directory(\n",
    "    CLEANED_DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# The train_generator and validation_generator now have normalized and augmented batches ready for model training.\n",
    "\n",
    "print(f\"Number of classes: {len(train_generator.class_indices)}\")\n",
    "print(f\"Training samples: {train_generator.samples}\")\n",
    "print(f\"Validation samples: {validation_generator.samples}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "1b0c542c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train-test split completed and data saved.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import shutil\n",
    "import random\n",
    "\n",
    "# Constants\n",
    "DATASET_DIR = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3'  # Path to your cleaned dataset folder\n",
    "TRAIN_DIR = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3_train'\n",
    "TEST_DIR = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3_test'\n",
    "TEST_SIZE = 0.15  # 15% for testing\n",
    "\n",
    "# Create directories if they don't exist\n",
    "os.makedirs(TRAIN_DIR, exist_ok=True)\n",
    "os.makedirs(TEST_DIR, exist_ok=True)\n",
    "\n",
    "# For each class folder in the dataset\n",
    "for class_folder in os.listdir(DATASET_DIR):\n",
    "    class_path = os.path.join(DATASET_DIR, class_folder)\n",
    "    if not os.path.isdir(class_path):\n",
    "        continue\n",
    "\n",
    "    images = os.listdir(class_path)\n",
    "    random.shuffle(images)\n",
    "\n",
    "    # Calculate split index\n",
    "    split_idx = int(len(images) * (1 - TEST_SIZE))\n",
    "    train_images = images[:split_idx]\n",
    "    test_images = images[split_idx:]\n",
    "\n",
    "    # Create class folders in train and test directories\n",
    "    train_class_path = os.path.join(TRAIN_DIR, class_folder)\n",
    "    test_class_path = os.path.join(TEST_DIR, class_folder)\n",
    "    os.makedirs(train_class_path, exist_ok=True)\n",
    "    os.makedirs(test_class_path, exist_ok=True)\n",
    "\n",
    "    # Move images to train folder\n",
    "    for img in train_images:\n",
    "        shutil.copy(os.path.join(class_path, img), train_class_path)\n",
    "\n",
    "    # Move images to test folder\n",
    "    for img in test_images:\n",
    "        shutil.copy(os.path.join(class_path, img), test_class_path)\n",
    "\n",
    "print(\"Train-test split completed and data saved.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "c263db13",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 71 images belonging to 27 classes.\n",
      "Found 1 images belonging to 27 classes.\n",
      "Epoch 1/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m13s\u001b[0m 2s/step - accuracy: 0.0563 - loss: 3.4461 - val_accuracy: 0.0000e+00 - val_loss: 3.3821\n",
      "Epoch 2/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 432ms/step - accuracy: 0.0986 - loss: 3.1252 - val_accuracy: 0.0000e+00 - val_loss: 3.1036\n",
      "Epoch 3/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 652ms/step - accuracy: 0.2817 - loss: 2.8682 - val_accuracy: 0.0000e+00 - val_loss: 3.0407\n",
      "Epoch 4/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 449ms/step - accuracy: 0.3803 - loss: 2.6699 - val_accuracy: 0.0000e+00 - val_loss: 3.1214\n",
      "Epoch 5/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 470ms/step - accuracy: 0.4648 - loss: 2.5379 - val_accuracy: 0.0000e+00 - val_loss: 2.9034\n",
      "Epoch 6/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 700ms/step - accuracy: 0.5775 - loss: 2.3322 - val_accuracy: 1.0000 - val_loss: 2.7180\n",
      "Epoch 7/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 769ms/step - accuracy: 0.6901 - loss: 2.1446 - val_accuracy: 0.0000e+00 - val_loss: 2.6041\n",
      "Epoch 8/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 496ms/step - accuracy: 0.7887 - loss: 1.9784 - val_accuracy: 0.0000e+00 - val_loss: 2.4472\n",
      "Epoch 9/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 768ms/step - accuracy: 0.8451 - loss: 1.8024 - val_accuracy: 0.0000e+00 - val_loss: 2.7216\n",
      "Epoch 10/10\n",
      "\u001b[1m3/3\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 774ms/step - accuracy: 0.8451 - loss: 1.6232 - val_accuracy: 0.0000e+00 - val_loss: 2.4422\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.src.callbacks.history.History at 0x1694f19c440>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import load_model, Model\n",
    "from tensorflow.keras.layers import Dense, Dropout, GlobalAveragePooling2D\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "# Constants\n",
    "DATASET_PATH = 'C:/Users/Ayush/ath_codee/cleaned_dataset_3'\n",
    "OLD_MODEL_PATH = 'C:/Users/Ayush/ath_codee/1_CNN_MODEL_code/efficientnetb0_plant_species.h5'\n",
    "BATCH_SIZE = 32\n",
    "IMAGE_SIZE = (224, 224)\n",
    "EPOCHS = 10\n",
    "\n",
    "# Load your existing trained model (with included pooling and classifier)\n",
    "base_model = load_model(OLD_MODEL_PATH)\n",
    "\n",
    "# Create data generators\n",
    "datagen = ImageDataGenerator(\n",
    "    preprocessing_function=tf.keras.applications.efficientnet.preprocess_input,\n",
    "    validation_split=0.2,\n",
    "    rotation_range=20,\n",
    "    zoom_range=0.2,\n",
    "    horizontal_flip=True\n",
    ")\n",
    "\n",
    "train_generator = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='training',\n",
    "    shuffle=True,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "validation_generator = datagen.flow_from_directory(\n",
    "    DATASET_PATH,\n",
    "    target_size=IMAGE_SIZE,\n",
    "    batch_size=BATCH_SIZE,\n",
    "    class_mode='categorical',\n",
    "    subset='validation',\n",
    "    shuffle=False,\n",
    "    seed=42\n",
    ")\n",
    "\n",
    "# Get correct number of classes dynamically\n",
    "num_classes = len(train_generator.class_indices)\n",
    "\n",
    "# Remove last Dense layer from base_model and get second last layer output\n",
    "x = base_model.layers[-3].output  # Dropout layer output (before Dense)\n",
    "\n",
    "# Add new Dense layer for new number of classes\n",
    "new_output = Dense(num_classes, activation='softmax')(x)\n",
    "\n",
    "# Create new model\n",
    "model = Model(inputs=base_model.input, outputs=new_output)\n",
    "\n",
    "# Freeze base layers initially\n",
    "for layer in model.layers[:-1]:\n",
    "    layer.trainable = False\n",
    "\n",
    "# Compile model\n",
    "model.compile(optimizer='adam', loss='categorical_crossentropy', metrics=['accuracy'])\n",
    "\n",
    "# Train the model\n",
    "model.fit(\n",
    "    train_generator,\n",
    "    validation_data=validation_generator,\n",
    "    epochs=EPOCHS\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "65a2758d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model saved as new_efficientnetb0_disease_detector.keras\n"
     ]
    }
   ],
   "source": [
    "model.save('new_efficientnetb0_disease_detector.keras')\n",
    "print(\"Model saved as new_efficientnetb0_disease_detector.keras\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "7d33997a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional_2\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"functional_2\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_5 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetb0 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)     â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1280</span>)           â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”‚ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">27</span>)             â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">34,587</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ input_layer_5 (\u001b[38;5;33mInputLayer\u001b[0m)      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ efficientnetb0 (\u001b[38;5;33mFunctional\u001b[0m)     â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1280\u001b[0m)     â”‚     \u001b[38;5;34m4,049,571\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ global_average_pooling2d_1      â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1280\u001b[0m)           â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”‚ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)        â”‚                        â”‚               â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m27\u001b[0m)             â”‚        \u001b[38;5;34m34,587\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,153,334</span> (15.84 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m4,153,334\u001b[0m (15.84 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">34,587</span> (135.11 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m34,587\u001b[0m (135.11 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">4,049,571</span> (15.45 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m4,049,571\u001b[0m (15.45 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Optimizer params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">69,176</span> (270.22 KB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Optimizer params: \u001b[0m\u001b[38;5;34m69,176\u001b[0m (270.22 KB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "# Replace the path below with your actual model file path\n",
    "model_path = 'new_efficientnetb0_disease_detector.keras'  # or .h5 if using legacy format\n",
    "\n",
    "# Load the model\n",
    "model = tf.keras.models.load_model(model_path)\n",
    "\n",
    "# Print model summary\n",
    "model.summary()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
